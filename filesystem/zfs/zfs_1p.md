

### 1PB ZFS 极速架构备忘录（含 3-way Special VDEV）

- **核心：** **dRAID2** 提供秒级逻辑重建，抗多盘损，空间利用率高。
- **元数据命门：** 必须用 **3-way Mirror NVMe (2TB+)** 组建 **Special VDEV**。
  - **注意：** Special VDEV 若全损则全池崩溃，3 路镜像可保“两盘同时坏”而不丢数据。
  - **进阶：** 开启 `small_blocks` 选项，让 4K/16K 小文件彻底脱离机械盘，直存 SSD。
- **缓存：** **128GB+ ARC** (RAM) 负责实时索引，**L2ARC** (SSD) 负责大容量二次加速。
- **连接：** 部署 **万兆（10GbE）** 及以上网络。

------

##### SMB 性能表现与调优提示

- **响应速度：** 元数据固化在 NVMe 镜像后，Windows 开启含有万级文件的文件夹可实现**“瞬间平铺”**，彻底告别进度条。
- **吞吐极限：** 顺序读写稳定在 **1.1GB/s**（万兆上限）。开启 **SMB Multichannel**（多通道）后可翻倍。
- **运维注意：** 1.  **SSD 寿命：** 务必选 **高 DWPD（企业级）** 颗粒，元数据写入频繁，普通消费级 SSD 易早衰。 2.  **网络同步：** 建议开启 `Large MTU (9000)`（巨型帧）以降低万兆下的 CPU 负载。 3.  **断电保护：** 3-way SSD 建议带 **PLP（掉电保护）** 电容，防止极端断电导致的文件描述符丢失。



### 硬件连接

连接千盘阵列，你主要会用到以下三类线：

| **线缆类型**             | **连接对象**                           | **说明**                                                  |
| ------------------------ | -------------------------------------- | --------------------------------------------------------- |
| **SFF-8644 to SFF-8644** | **HBA 卡 ↔ 磁盘柜** 或 **柜子 ↔ 柜子** | 外部高速 SAS 线，用于柜间级联，通常支持 12Gbps × 4 宽度。 |
| **SFF-8643 to SFF-8643** | **机箱内 Expander ↔ 背板**             | 内部 SAS 线，用于柜子内部的信号分发。                     |
| **SFF-8643 to 4x SATA**  | **Expander ↔ 单个硬盘**                | 如果没有背板，则需要这种“一拖四”的分离线直接插硬盘。      |





**ZFS 的局限：** ZFS 是单机王，但无法原生跨越数千台服务器形成统一的 PB/EB 级命名空间。









### 文件系统总结对比

| **特性**        | **ZFS (TrueNAS)**       | **Btrfs**        | **ext4**         | **Ceph**         |
| --------------- | ----------------------- | ---------------- | ---------------- | ---------------- |
| **自我修复**    | ✅ 极其完善              | ⚠️ 有，但不稳     | ❌ 无             | ✅ 极其完善       |
| **RAID 安全性** | ✅ 工业级 dRAID/Z2       | ❌ RAID5/6 有风险 | ❌ 依赖外部 RAID  | ✅ 节点级冗余     |
| **快照功能**    | ✅ 秒级、无限量          | ✅ 支持           | ❌ 依赖 LVM       | ✅ 支持           |
| **上手难度**    | 🟢 简单 (TrueNAS)        | 🟡 中等           | 🟢 简单           | 🔴 极难           |
| **最佳场景**    | **单机 10T - 1PB 存储** | 桌面级 Linux     | 系统盘、轻量应用 | 多机分布式云存储 |





| **特性**               | **ext4**                   | **Btrfs**                  |
| ---------------------- | -------------------------- | -------------------------- |
| **元数据位翻转检测**   | ✅ 有（保护目录结构）       | ✅ 有                       |
| **文件数据位翻转检测** | ❌ **无**                   | ✅ **有**                   |
| **静默损坏自修复**     | ❌ 无                       | ✅ 有（需 RAID 配合）       |
| **设计逻辑**           | 简单、极速、不操心数据变质 | 现代、安全、时刻防范位翻转 |



## 1. 为什么“传统 RAID”正在过时？

传统的硬件 RAID 或 `mdadm` 主要有三个痛点：

- **静默数据损坏（Bit Rot）：** 随着数据量达到 100T，文件发生位翻转的概率几乎是 100%。传统 RAID 无法检测到这种错误，只会把错误的数据读给你。
- **硬件依赖：** 硬件 RAID 卡一旦损坏，你必须找一块同型号的卡才能恢复数据，增加了单点故障风险。
- **重构风险：** 正如我们之前讨论的，传统 RAID 在重构大容量硬盘时，极易因 URE（不可恢复错误）导致整个阵列崩溃。











### 调研



##### 100T 方案选择指南

| **需求场景**                   | **推荐方案**               | **核心理由**                               |
| ------------------------------ | -------------------------- | ------------------------------------------ |
| **极致安全、高性能、数据自愈** | **ZFS (RAID-Z2/Z3)**       | 存储界的“劳斯莱斯”，防止一切数据损坏。     |
| **混合不同容量的旧硬盘、省电** | **SnapRAID + MergerFS**    | 灵活度最高，坏盘后数据不会全丢。           |
| **多台服务器横向扩展**         | **Ceph / MinIO**           | 适合追求分布式架构的开发者。               |
| **追求极致简单、性能**         | **`mdadm` (RAID 6) + XFS** | 传统但高效，适合不担心位翻转的纯读取场景。 |





虽然 GlusterFS 理论上能支持到 EB 级，但在 CERN 的极限环境下，它有几个“短板”：

- **小文件性能：** 当文件数量达到数十亿级时，GlusterFS 的“无元数据”架构会导致某些操作（如 `ls` 扫描大目录）变慢，而 CERN 自研的 EOS 针对这种海量并发做了专项优化。
- **硬件开销：** GlusterFS 倾向于通过“副本（Replica）”来保安全，存 1PB 数据可能需要 2PB-3PB 的空间；而 CERN 为了省钱省地儿，更依赖纠删码（Erasure Coding）技术，这点在 EOS 上实现得更早、更成熟。



**欧洲核子研究中心**

自研的分布式存储系统：EOS